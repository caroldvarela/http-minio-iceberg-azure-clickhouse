{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf0f6287",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyiceberg.catalog import load_catalog\n",
    "import pyarrow as pa\n",
    "import dlt\n",
    "import pandas as pd\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.fs as fs\n",
    "from dlt.sources.filesystem import filesystem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5835bd",
   "metadata": {},
   "source": [
    "## **Connect to the Nessie catalog**\n",
    "\n",
    "We reconnect to the **Nessie REST catalog** to access the Iceberg tables created in the previous notebook.\n",
    "\n",
    "Listing the namespaces ensures that the connection is active and the catalog is available for querying.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda17a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespaces: [('taxis-project',)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configure the connection to the Nessie REST catalog\n",
    "catalog = load_catalog(\n",
    "    \"nessie\",\n",
    "    **{\n",
    "        \"uri\": \"http://nessie:19120/iceberg/main/\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Verify the connection by listing the namespaces\n",
    "namespaces = catalog.list_namespaces()\n",
    "print(\"Namespaces:\", namespaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056b1c79",
   "metadata": {},
   "source": [
    "## **Extract Iceberg data and load it into Azure Storage**\n",
    "\n",
    "This step performs the **third stage** of the data pipeline —  \n",
    "**moving the Iceberg data stored in MinIO into Azure Blob Storage** using DLT.\n",
    "\n",
    "**Process overview:**\n",
    "\n",
    "1. **Configure the MinIO filesystem**  \n",
    "   - Establish an S3-compatible connection to MinIO with credentials and endpoint details.\n",
    "\n",
    "2. **Define the DLT resource (`iceberg_df`)**  \n",
    "   - Load the `taxis-project.taxis` table from the Nessie catalog.  \n",
    "   - Execute a scan to retrieve the data as an Arrow Table.  \n",
    "   - Yield the data in Arrow batches to be consumed by DLT.\n",
    "\n",
    "3. **Initialize the pipeline**  \n",
    "   - Name: `s3_to_adls`  \n",
    "   - Destination: `filesystem` (configured for Azure via secrets).  \n",
    "   - Dataset name: `azure`.\n",
    "\n",
    "4. **Run the pipeline**  \n",
    "   - Writes the extracted data from Iceberg (MinIO) into Azure ADLS in **Parquet** format.  \n",
    "   - Uses `write_disposition=\"replace\"` to overwrite any existing dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26243b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline s3_to_adls load step completed in 1 minute and 38.66 seconds\n",
      "1 load package(s) were loaded to destination filesystem and into dataset azure\n",
      "The filesystem destination used abfss://clase-4-dlt@fhbd.dfs.core.windows.net/GRUPO_4 location to store data\n",
      "Load package 1757017205.9115121 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "s3 = fs.S3FileSystem(\n",
    "    endpoint_override=\"http://minio:9000\",  # inside Docker: use \"minio:9000\", from local: \"localhost:9000\"\n",
    "    access_key=\"admin\",\n",
    "    secret_key=\"password\",\n",
    "    region=\"us-east-1\"\n",
    ")\n",
    "\n",
    "iceberg_table_path = \"my-bucket/taxis-project/taxis\"\n",
    "\n",
    "@dlt.resource(table_name=\"taxis\")\n",
    "def iceberg_df():\n",
    "    # Load the Iceberg table from the Nessie catalog\n",
    "    taxis = catalog.load_table(\"taxis-project.taxis\")\n",
    "    \n",
    "    # Execute the scan and get an Arrow Table\n",
    "    arrow_table = taxis.scan().to_arrow()\n",
    "    \n",
    "    # Iterate over Arrow batches\n",
    "    for batch in arrow_table.to_batches():\n",
    "        yield batch\n",
    "\n",
    "# Define the DLT pipeline\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"s3_to_adls\",\n",
    "    destination=\"filesystem\",\n",
    "    dataset_name=\"azure\"\n",
    ")\n",
    "\n",
    "# Define the source (filesystem connector)\n",
    "source = filesystem()\n",
    "\n",
    "# Run the pipeline\n",
    "load_info = pipeline.run(\n",
    "    iceberg_df,             # indicates: use the filesystem connector\n",
    "    loader_file_format=\"parquet\",\n",
    "    write_disposition=\"replace\"\n",
    ")\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f9cf84",
   "metadata": {},
   "source": [
    "The pipeline **successfully transferred data** from MinIO to Azure Blob Storage.\n",
    "\n",
    "**Key details:**\n",
    "- Runtime: ~1 minute 38 seconds.  \n",
    "- Destination: Azure ADLS (`abfss://clase-4-dlt@fhbd.dfs.core.windows.net/GRUPO_4`).  \n",
    "- Dataset: `azure`.  \n",
    "- No failed jobs were detected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec7e05c",
   "metadata": {},
   "source": [
    "## **Verify the uploaded files in Azure**\n",
    "\n",
    "After the DLT pipeline finishes, we connect directly to **Azure Data Lake Storage (ADLS)**  \n",
    "using `fsspec` to verify that the Parquet files were successfully uploaded.\n",
    "\n",
    "Steps:\n",
    "1. Retrieve Azure credentials (account name and key) from the DLT secrets.  \n",
    "2. Initialize an `abfss` filesystem connection.  \n",
    "3. List all files under the `azure` dataset path to confirm the data transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e32f81f5-dfee-40cd-be16-da3d571a44e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos en Azure: ['clase-4-dlt/GRUPO_4/azure/_dlt_loads', 'clase-4-dlt/GRUPO_4/azure/_dlt_pipeline_state', 'clase-4-dlt/GRUPO_4/azure/_dlt_version', 'clase-4-dlt/GRUPO_4/azure/filesystem', 'clase-4-dlt/GRUPO_4/azure/init', 'clase-4-dlt/GRUPO_4/azure/post_2020', 'clase-4-dlt/GRUPO_4/azure/taxis']\n"
     ]
    }
   ],
   "source": [
    "import fsspec\n",
    "secrets = dlt.secrets[\"s3_to_adls\"][\"destination\"][\"filesystem\"]\n",
    "\n",
    "path = secrets[\"bucket_url\"] + \"/azure\"\n",
    "account_name = secrets[\"credentials\"][\"azure_storage_account_name\"]\n",
    "account_key = secrets[\"credentials\"][\"azure_storage_account_key\"]\n",
    "\n",
    "fs = fsspec.filesystem(\n",
    "    \"abfss\",\n",
    "    account_name=account_name,       # tu cuenta de storage\n",
    "    credential=account_key\n",
    ")\n",
    "\n",
    "# Listar los archivos que subió DLT\n",
    "files = fs.ls(path)\n",
    "print(\"Archivos en Azure:\", files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e00eac0",
   "metadata": {},
   "source": [
    "The output confirms that multiple DLT-related folders and datasets were created in Azure, including:\n",
    "- `_dlt_loads`, `_dlt_pipeline_state`, `_dlt_version` → pipeline metadata.\n",
    "- `taxis` → actual data folder.\n",
    "\n",
    "This verifies the **successful ingestion of the Iceberg data into Azure Blob Storage (ADLS)**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
