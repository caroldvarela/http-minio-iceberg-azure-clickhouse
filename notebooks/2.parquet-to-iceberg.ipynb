{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cceb935",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyiceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c609066a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.fs as fs\n",
    "import pyarrow.dataset as ds\n",
    "from pyiceberg.catalog import load_catalog\n",
    "from pyiceberg.schema import Schema, NestedField\n",
    "from pyiceberg.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6691d7",
   "metadata": {},
   "source": [
    "## **Connect to the Nessie REST catalog**\n",
    "\n",
    "We initialize a connection to the **Nessie Catalog** through its REST API.\n",
    "\n",
    "This catalog acts as the **metadata management layer** for Iceberg tables.  \n",
    "All table versions, namespaces, and schema changes will be tracked here.\n",
    "\n",
    "We verify the connection by listing existing namespaces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76362cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespaces: []\n"
     ]
    }
   ],
   "source": [
    "# Configurar la conexión al catálogo REST de Nessie\n",
    "catalog = load_catalog(\n",
    "    \"nessie\",\n",
    "    **{\n",
    "        \"uri\": \"http://nessie:19120/iceberg/main/\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Verificar conexion listando los namespaces\n",
    "namespaces = catalog.list_namespaces()\n",
    "print(\"Namespaces:\", namespaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62153de",
   "metadata": {},
   "source": [
    "### **Create a new namespace in Nessie**\n",
    "\n",
    "Since no namespaces were found, we create a new one named **`taxis-project`**  \n",
    "to organize our Iceberg tables under this logical grouping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec830f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.create_namespace(\"taxis-project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e184f2e",
   "metadata": {},
   "source": [
    "We confirm that the `taxis-project` namespace was successfully created by listing all available namespaces again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946c21d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespaces: [('taxis-project',)]\n"
     ]
    }
   ],
   "source": [
    "# Listar namespaces\n",
    "namespaces = catalog.list_namespaces()\n",
    "print(\"Namespaces:\", namespaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab70d2b",
   "metadata": {},
   "source": [
    "## **Connect to the MinIO storage (S3-compatible)**\n",
    "\n",
    "We configure access to **MinIO**, which acts as an S3-compatible object storage.  \n",
    "This connection allows us to browse and read Parquet files that were uploaded in the previous step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59acec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = fs.S3FileSystem(\n",
    "    access_key=\"admin\",\n",
    "    secret_key=\"password\",\n",
    "    endpoint_override=\"http://minio:9000\",\n",
    "    region=\"us-east-1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308ec778",
   "metadata": {},
   "source": [
    "We specify the folder containing the previously ingested dataset and list all the files available in that location.\n",
    "\n",
    "- Only `.parquet` files are filtered.\n",
    "- If none are found, an exception is raised.\n",
    "- The first available Parquet file is selected for conversion to Iceberg format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7bed863b-78c2-47c1-934b-d91f0d3219dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found: taxis/taxis_parquet/df_data/1761058478.437103.46027fda46.parquet\n"
     ]
    }
   ],
   "source": [
    "# Base folder in the bucket\n",
    "folder_path = \"taxis/taxis_parquet/df_data/\"\n",
    "\n",
    "# List the files in the folder\n",
    "files = s3.get_file_info(fs.FileSelector(folder_path))\n",
    "\n",
    "# Filter only Parquet files\n",
    "parquet_files = [f.path for f in files if f.is_file and f.path.endswith(\".parquet\")]\n",
    "\n",
    "if not parquet_files:\n",
    "    raise FileNotFoundError(f\"No .parquet files found in .parquet en {folder_path}\")\n",
    "\n",
    "# Use the first Parquet file found\n",
    "path = parquet_files[0]\n",
    "\n",
    "print(\"File found:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dce48c8",
   "metadata": {},
   "source": [
    "### **Read the Parquet file into a PyArrow Table**\n",
    "\n",
    "We create a **PyArrow dataset** from the selected Parquet file using the S3 filesystem.  \n",
    "Then, we load it into memory as an Arrow `Table` — this structure will later be mapped to an Iceberg schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fffeb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ds.dataset(path, filesystem=s3, format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f69294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = dataset.to_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359dcda0",
   "metadata": {},
   "source": [
    "We print the schema of the loaded table to understand its structure and data types.  \n",
    "This step is necessary to define an equivalent schema for the Iceberg table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca74faea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vendor_id: int32\n",
      "tpep_pickup_datetime: timestamp[us]\n",
      "tpep_dropoff_datetime: timestamp[us]\n",
      "passenger_count: double\n",
      "trip_distance: double\n",
      "ratecode_id: double\n",
      "store_and_fwd_flag: string\n",
      "pu_location_id: int32\n",
      "do_location_id: int32\n",
      "payment_type: int64\n",
      "fare_amount: double\n",
      "extra: double\n",
      "mta_tax: double\n",
      "tip_amount: double\n",
      "tolls_amount: double\n",
      "improvement_surcharge: double\n",
      "total_amount: double\n",
      "congestion_surcharge: double\n",
      "airport_fee: double\n",
      "cbd_congestion_fee: double\n"
     ]
    }
   ],
   "source": [
    "table_schema = table.schema\n",
    "print(table_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220edd51",
   "metadata": {},
   "source": [
    "## **Define a conversion function: Arrow → Iceberg Schema**\n",
    "\n",
    "We implement a helper function `arrow_to_iceberg()` that converts a PyArrow schema  \n",
    "into a valid Iceberg schema by mapping each Arrow type to its Iceberg equivalent.\n",
    "\n",
    "This function iterates through all fields and creates a list of `NestedField` objects,  \n",
    "ensuring type consistency across both systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bcd41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrow_to_iceberg(arrow_schema: pa.Schema) -> Schema:\n",
    "    fields = []\n",
    "    field_id = 1\n",
    "\n",
    "    for field in arrow_schema:\n",
    "        name = field.name\n",
    "        arrow_type = field.type\n",
    "\n",
    "        if pa.types.is_int64(arrow_type):\n",
    "            iceberg_type = LongType()\n",
    "        elif pa.types.is_int32(arrow_type):\n",
    "            iceberg_type = IntegerType()\n",
    "        elif pa.types.is_float32(arrow_type):\n",
    "            iceberg_type = FloatType()\n",
    "        elif pa.types.is_float64(arrow_type):\n",
    "            iceberg_type = DoubleType()\n",
    "        elif pa.types.is_string(arrow_type):\n",
    "            iceberg_type = StringType()\n",
    "        elif pa.types.is_binary(arrow_type):\n",
    "            iceberg_type = BinaryType()\n",
    "        elif pa.types.is_timestamp(arrow_type):\n",
    "            if arrow_type.tz:\n",
    "                iceberg_type = TimestamptzType()\n",
    "            else:\n",
    "                iceberg_type = TimestampType()\n",
    "        else:\n",
    "            raise ValueError(f\"Tipo Arrow no soportado todavía: {arrow_type}\")\n",
    "\n",
    "        fields.append(NestedField(field_id, name, iceberg_type, required=not field.nullable))\n",
    "        field_id += 1\n",
    "\n",
    "    return Schema(*fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7812180",
   "metadata": {},
   "source": [
    "We call `arrow_to_iceberg()` to automatically generate an Iceberg-compatible schema  \n",
    "based on the structure of the original Parquet file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8d4f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_schema = arrow_to_iceberg(table_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169e9da7",
   "metadata": {},
   "source": [
    "### **Create the Iceberg table in Nessie**\n",
    "\n",
    "We create a new Iceberg table named **`taxis-project.taxis`** in the Nessie catalog,  \n",
    "using the previously generated schema.\n",
    "\n",
    "If the table already exists, it will not be recreated to avoid conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0457da",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not catalog.table_exists(\"taxis-project.taxis\"):\n",
    "    catalog.create_table(\"taxis-project.taxis\", schema=post_schema)\n",
    "else:\n",
    "    print(\"The table taxis-project.taxis already exists, it was not created again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9df59b",
   "metadata": {},
   "source": [
    "### **Append Parquet data to the Iceberg table**\n",
    "\n",
    "Finally, we load the Iceberg table from the catalog and **append** the Parquet data.  \n",
    "This writes the dataset into MinIO following the Iceberg table structure,  \n",
    "enabling schema evolution, partitioning, and metadata tracking via Nessie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44f632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxis  = catalog.load_table(\"taxis-project.taxis\")\n",
    "taxis.append(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
